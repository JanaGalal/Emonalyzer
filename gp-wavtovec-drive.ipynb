{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Dataset from Google Drive","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install datasets transformers torchaudio","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"! pip install audiomentations","metadata":{"execution":{"iopub.status.busy":"2024-06-29T16:20:53.332191Z","iopub.execute_input":"2024-06-29T16:20:53.332526Z","iopub.status.idle":"2024-06-29T16:21:07.447649Z","shell.execute_reply.started":"2024-06-29T16:20:53.332496Z","shell.execute_reply":"2024-06-29T16:21:07.446585Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting audiomentations\n  Downloading audiomentations-0.36.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from audiomentations) (1.26.4)\nRequirement already satisfied: librosa!=0.10.0,<0.11.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from audiomentations) (0.10.2.post1)\nRequirement already satisfied: scipy<1.13,>=1.4 in /opt/conda/lib/python3.10/site-packages (from audiomentations) (1.11.4)\nRequirement already satisfied: soxr<1.0.0,>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from audiomentations) (0.3.7)\nRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.4.2)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (5.1.1)\nRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.58.1)\nRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.12.1)\nRequirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.8.1)\nRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (4.9.0)\nRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.3)\nRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.0.7)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (0.41.1)\nRequirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.11.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (21.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.32.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.2.0)\nRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.16.0)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2.21)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa!=0.10.0,<0.11.0,>=0.8.0->audiomentations) (2024.2.2)\nDownloading audiomentations-0.36.0-py3-none-any.whl (80 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: audiomentations\nSuccessfully installed audiomentations-0.36.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install gdown","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-30T11:17:26.986971Z","iopub.execute_input":"2024-06-30T11:17:26.987686Z","iopub.status.idle":"2024-06-30T11:17:39.804510Z","shell.execute_reply.started":"2024-06-30T11:17:26.987653Z","shell.execute_reply":"2024-06-30T11:17:39.803549Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nUsing cached gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import gdown\n# Replace with your own file ID\nfile_id = '1HUK5kT54xWiy41bqQTiIe-fF8sOj7yvS'\ngdown.download(f'https://drive.google.com/uc?id={file_id}', 'final_output_vanilla_EYASE_EAED.zip', quiet=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:17:39.806530Z","iopub.execute_input":"2024-06-30T11:17:39.806815Z","iopub.status.idle":"2024-06-30T11:17:58.169149Z","shell.execute_reply.started":"2024-06-30T11:17:39.806788Z","shell.execute_reply":"2024-06-30T11:17:58.168234Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1HUK5kT54xWiy41bqQTiIe-fF8sOj7yvS\nFrom (redirected): https://drive.google.com/uc?id=1HUK5kT54xWiy41bqQTiIe-fF8sOj7yvS&confirm=t&uuid=d7e3eecb-40e5-45ff-8fc4-f17f73212901\nTo: /kaggle/working/final_output_vanilla_EYASE_EAED.zip\n100%|██████████| 662M/662M [00:16<00:00, 40.9MB/s] \n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'final_output_vanilla_EYASE_EAED.zip'"},"metadata":{}}]},{"cell_type":"code","source":"!unzip -q final_output_vanilla_EYASE_EAED.zip ","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:17:58.170496Z","iopub.execute_input":"2024-06-30T11:17:58.171035Z","iopub.status.idle":"2024-06-30T11:18:07.914267Z","shell.execute_reply.started":"2024-06-30T11:17:58.170999Z","shell.execute_reply":"2024-06-30T11:18:07.913003Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport librosa\nimport torchaudio\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport seaborn as sns\nimport soundfile as sf\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom datasets import Dataset,load_metric\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n#from audiomentations import Compose, TimeStretch, PitchShift, AddBackgroundNoise\nfrom transformers import AutoProcessor, Wav2Vec2Processor, Wav2Vec2ForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, TrainerCallback\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:18:40.407580Z","iopub.execute_input":"2024-06-30T11:18:40.408554Z","iopub.status.idle":"2024-06-30T11:18:53.068535Z","shell.execute_reply.started":"2024-06-30T11:18:40.408513Z","shell.execute_reply":"2024-06-30T11:18:53.067749Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"2024-06-30 11:18:42.898656: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-30 11:18:42.898763: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-30 11:18:43.026995: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Spliting data set to train and test","metadata":{}},{"cell_type":"code","source":"data_dir = 'final_output_vanilla_EYASE_EAED'\n\n# List all files in the directory\nfiles = os.listdir(data_dir)\nprint(f'Total files: {len(files)}')\nprint(files[:10])  # Print the first 10 files as a sample","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:19:23.416571Z","iopub.execute_input":"2024-06-30T11:19:23.416955Z","iopub.status.idle":"2024-06-30T11:19:23.425152Z","shell.execute_reply.started":"2024-06-30T11:19:23.416925Z","shell.execute_reply":"2024-06-30T11:19:23.424230Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Total files: 3541\n['FEM_EnasKamel_ang_2075.wav', 'MAL_AsserYassin_neu_2682.wav', 'MAL_KhaledSarhan_sad_1640.wav', 'FEM_Zeina_sad_856.wav', 'FEM__EYASE_ang_100016.wav', 'MAL_YoussefSherief_neu_2466.wav', 'MAL__EYASE_neu_100418.wav', 'FEM_Lebleba_ang_1446.wav', 'FEM_NadiaKhairy_sad_1529.wav', 'MAL__EYASE_neu_100323.wav']\n","output_type":"stream"}]},{"cell_type":"code","source":"# List all audio files in the directory\naudio_files = [os.path.join(data_dir, file) for file in os.listdir(data_dir) if file.endswith('.wav')]\nemotions = [file.split('_')[-2] for file in audio_files]\n\n# Splitting into train (70%), validation (15%), and test (15%)\ntrain_files, test_files, train_emotions, test_emotions = train_test_split(audio_files, emotions, test_size=0.3, random_state=42, stratify=emotions)\nval_files, test_files, val_emotions, test_emotions = train_test_split(test_files, test_emotions, test_size=0.5, random_state=42, stratify=test_emotions)\n\nprint(f'Training files: {len(train_files)}')\nprint(f'Validation files: {len(val_files)}')\nprint(f'Test files: {len(test_files)}')\n\n# Convert the list to a pandas Series for easy counting\nemotions_series = pd.Series(emotions)\n\n# Count occurrences of each emotion\nemotion_counts = emotions_series.value_counts()\n\n# Display the counts\nprint(\"Emotion Counts:\")\nprint(emotion_counts)\n\nprint(f\"Train emotions distribution: {dict(zip(*np.unique(train_emotions, return_counts=True)))}\")\nprint(f\"Validation emotions distribution: {dict(zip(*np.unique(val_emotions, return_counts=True)))}\")\nprint(f\"Test emotions distribution: {dict(zip(*np.unique(test_emotions, return_counts=True)))}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:19:27.085702Z","iopub.execute_input":"2024-06-30T11:19:27.086849Z","iopub.status.idle":"2024-06-30T11:19:27.130107Z","shell.execute_reply.started":"2024-06-30T11:19:27.086786Z","shell.execute_reply":"2024-06-30T11:19:27.129091Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Training files: 2478\nValidation files: 531\nTest files: 532\nEmotion Counts:\nneu    1154\nang    1059\nsad     768\nhap     560\nName: count, dtype: int64\nTrain emotions distribution: {'ang': 741, 'hap': 392, 'neu': 808, 'sad': 537}\nValidation emotions distribution: {'ang': 159, 'hap': 84, 'neu': 173, 'sad': 115}\nTest emotions distribution: {'ang': 159, 'hap': 84, 'neu': 173, 'sad': 116}\n","output_type":"stream"}]},{"cell_type":"code","source":"'''# Convert the list to a pandas Series for easy counting\ntrain_emotions_series = pd.Series(train_emotions)\n\n# Count occurrences of each emotion\ntrain_emotion_counts = train_emotions_series.value_counts()\n# Create directories for augmented files\ntrain_aug_dir = os.path.join(data_dir, 'train_augmented')\nos.makedirs(train_aug_dir, exist_ok=True)\n\n# Augmentation function\ndef augment_audio(input_file, output_file, time_stretch_factor=0.2, pitch_shift_steps=1, noise_path='path_to_noise_files'):\n    augment = Compose([\n        TimeStretch(min_rate=0.8, max_rate=1.2),\n        PitchShift(min_semitones=-pitch_shift_steps, max_semitones=pitch_shift_steps),\n        AddBackgroundNoise(sounds_path=noise_path)\n    ])\n\n    # Load audio file and apply augmentation\n    audio, sample_rate = librosa.load(input_file, sr=None)\n    augmented_audio = augment(samples=audio, sample_rate=sample_rate)\n\n    # Save augmented audio\n    sf.write(output_file, augmented_audio, sample_rate)\n\n# Augmentation function\ndef augment_audio(input_file, output_file, time_stretch_factor=0.2, pitch_shift_steps=1, noise_std=0.005):\n    augment = Compose([\n        TimeStretch(min_rate=0.8, max_rate=1.2),\n        PitchShift(min_semitones=-pitch_shift_steps, max_semitones=pitch_shift_steps)\n    ])\n\n    # Load audio file and apply augmentation\n    audio, sample_rate = librosa.load(input_file, sr=None)\n    augmented_audio = augment(samples=audio, sample_rate=sample_rate)\n\n    # Add Gaussian noise\n    noise = np.random.normal(0, noise_std, augmented_audio.shape)\n    augmented_audio = augmented_audio + noise\n\n    # Save augmented audio\n    sf.write(output_file, augmented_audio, sample_rate)\n\n# Function to augment a specific class\ndef augment_class(files, target_count, augment_dir):\n    count = len(files)\n    augmented_emotions = []\n    for i in range(target_count - count):\n        file = files[i % count]\n        file_name = os.path.basename(file)\n        aug_file_name = f\"aug_{i}_{file_name}\"\n        aug_file_path = os.path.join(augment_dir, aug_file_name)\n        augment_audio(file, aug_file_path)\n        # Append the corresponding emotion to augmented_emotions\n        augmented_emotions.append(train_emotions[train_files.index(file)])\n    \n    return augmented_emotions\n\n# Separate files by emotion\nhap_files = [file for file, emotion in zip(train_files, train_emotions) if emotion == 'hap']\nneu_files = [file for file, emotion in zip(train_files, train_emotions) if emotion == 'neu']\nsad_files = [file for file, emotion in zip(train_files, train_emotions) if emotion == 'sad']\n\n# Determine the target count for augmentation (using 'ang' as a reference)\ntarget_count = train_emotion_counts['ang']\nprint(target_count)\n\n# Augment \"hap\" and \"neu\" classes to balance them\nhap_augmented_emotions = augment_class(hap_files, target_count, train_aug_dir)\nneu_augmented_emotions = augment_class(neu_files, target_count, train_aug_dir)\nsad_augmented_emotions = augment_class(sad_files, target_count, train_aug_dir)\n\n# Append augmented files and their emotions to the training set\ntrain_files += hap_files\ntrain_files += neu_files\ntrain_files += sad_files\n\ntrain_emotions += hap_augmented_emotions\ntrain_emotions += neu_augmented_emotions\ntrain_emotions += sad_augmented_emotions\n\n# Convert the list to a pandas Series for easy counting\ntrain_emotions_series = pd.Series(train_emotions)\n\n# Count occurrences of each emotion in the augmented train set\naugmented_emotion_counts = train_emotions_series.value_counts()\n\n# Display the counts\nprint(\"Augmented Emotion Counts in Train Set:\")\nprint(augmented_emotion_counts)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Helper Functions","metadata":{}},{"cell_type":"code","source":"# Function to load and preprocess the audio files\ndef load_and_preprocess(file_path, target_sr=16000):\n    try:\n        # Load audio file\n        speech_array, sampling_rate = librosa.load(file_path, sr=target_sr)\n        \n        # Return the speech array\n        return speech_array\n    \n    except Exception as e:\n        print(f\"Error loading {file_path}: {str(e)}\")\n        return None\n\n# Create a DataFrame with file paths and labels\ndef create_dataframe(file_list):\n    data = []\n    for file_path in file_list:\n        # Extract label from filename (e.g., 'file_id_emotion.wav')\n        label = file_path.split('_')[-2].replace('.wav', '')\n        data.append({'file_path': file_path, 'label': label})\n    return pd.DataFrame(data)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:19:33.968311Z","iopub.execute_input":"2024-06-30T11:19:33.968666Z","iopub.status.idle":"2024-06-30T11:19:33.975343Z","shell.execute_reply.started":"2024-06-30T11:19:33.968637Z","shell.execute_reply":"2024-06-30T11:19:33.974411Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Create dataframes for training, validation, and test sets\ntrain_df = create_dataframe(train_files)\nval_df = create_dataframe(val_files)\ntest_df = create_dataframe(test_files)\n\n# Create datasets\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:19:35.963861Z","iopub.execute_input":"2024-06-30T11:19:35.964262Z","iopub.status.idle":"2024-06-30T11:19:36.013467Z","shell.execute_reply.started":"2024-06-30T11:19:35.964232Z","shell.execute_reply":"2024-06-30T11:19:36.012738Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(train_dataset)\nprint(val_dataset)\nprint(test_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:19:38.130799Z","iopub.execute_input":"2024-06-30T11:19:38.131170Z","iopub.status.idle":"2024-06-30T11:19:38.136264Z","shell.execute_reply.started":"2024-06-30T11:19:38.131139Z","shell.execute_reply":"2024-06-30T11:19:38.135335Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Dataset({\n    features: ['file_path', 'label'],\n    num_rows: 2478\n})\nDataset({\n    features: ['file_path', 'label'],\n    num_rows: 531\n})\nDataset({\n    features: ['file_path', 'label'],\n    num_rows: 532\n})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocess the Datasets","metadata":{}},{"cell_type":"code","source":"label2id = {'hap': 0, 'sad': 1, 'ang': 2, 'neu': 3}\n\nprocessor = AutoProcessor.from_pretrained(\"elgeish/wav2vec2-large-xlsr-53-arabic\")\n\ndef preprocess_function(examples):\n    audio_list = []\n    max_length = 0\n    \n    for file_path in examples['file_path']:\n        # Load the audio file\n        waveform, sample_rate = librosa.load(file_path, sr=16000)\n        \n        # Update the maximum length\n        if len(waveform) > max_length:\n            max_length = len(waveform)\n        \n        audio_list.append(waveform)\n    \n    # Pad/truncate all audio to the maximum length\n    padded_audio_list = []\n    for waveform in audio_list:\n        if len(waveform) > max_length:\n            waveform = waveform[:max_length]\n        else:\n            pad_length = max_length - len(waveform)\n            waveform = np.pad(waveform, (0, pad_length), 'constant')\n        padded_audio_list.append(waveform)\n    \n    # Convert padded audio list to PyTorch tensor\n    inputs = processor(padded_audio_list, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n    \n    # Convert labels to numerical IDs\n    labels = [label2id[label] for label in examples['label']]\n    \n    # Ensure labels are tensors\n    inputs['labels'] = torch.tensor(labels, dtype=torch.long)\n    \n    return inputs\n\n# Apply the preprocess function to the datasets\ntrain_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=['file_path'])\nval_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=['file_path'])\ntest_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=['file_path'])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:19:45.420650Z","iopub.execute_input":"2024-06-30T11:19:45.421016Z","iopub.status.idle":"2024-06-30T11:20:28.270367Z","shell.execute_reply.started":"2024-06-30T11:19:45.420986Z","shell.execute_reply":"2024-06-30T11:20:28.269383Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2478 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74d5e8f9a2f945edbaa8cce5dd902187"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/531 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"447e542adffc428d887da70b150aaa90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/532 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"595d661a7de14629abc2bc3aba525ed2"}},"metadata":{}}]},{"cell_type":"code","source":"# Verify labels are integers\nprint(type(train_dataset[0]['labels']))  # Should print <class 'int'>\nprint(type(val_dataset[0]['labels']))    # Should print <class 'int'>\nprint(type(test_dataset[0]['labels']))   # Should print <class 'int'>\n\n# Verify audio inputs are tensors\nprint(type(train_dataset[0]['input_values']))  # Should print <class 'torch.Tensor'>\nprint(type(val_dataset[0]['input_values']))    # Should print <class 'torch.Tensor'>\nprint(type(test_dataset[0]['input_values']))   # Should print <class 'torch.Tensor'>","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:20:28.272174Z","iopub.execute_input":"2024-06-30T11:20:28.272457Z","iopub.status.idle":"2024-06-30T11:20:28.861980Z","shell.execute_reply.started":"2024-06-30T11:20:28.272432Z","shell.execute_reply":"2024-06-30T11:20:28.860948Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"<class 'int'>\n<class 'int'>\n<class 'int'>\n<class 'list'>\n<class 'list'>\n<class 'list'>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Solve Data imbalance problem","metadata":{}},{"cell_type":"code","source":"# Define class weights based on the provided counts\nclass_counts = {\"ang\": 741, \"hap\": 392, \"neu\": 808, \"sad\": 537}\ntotal_samples = sum(class_counts.values())\nclass_weights = {cls: total_samples / count for cls, count in class_counts.items()}\nclass_weights_tensor = torch.tensor([class_weights[\"neu\"], class_weights[\"ang\"], class_weights[\"sad\"], class_weights[\"hap\"]], dtype=torch.float)\nprint(class_weights_tensor)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:20:28.863027Z","iopub.execute_input":"2024-06-30T11:20:28.863314Z","iopub.status.idle":"2024-06-30T11:20:28.916473Z","shell.execute_reply.started":"2024-06-30T11:20:28.863289Z","shell.execute_reply":"2024-06-30T11:20:28.915394Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"tensor([3.0668, 3.3441, 4.6145, 6.3214])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Custom Trainer to use class weights\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.class_weights = class_weights\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        \n        # Use class weights\n        class_weights = torch.tensor(self.class_weights).to(self.args.device)\n        loss = F.cross_entropy(logits, labels, weight=class_weights)\n        \n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:20:28.918325Z","iopub.execute_input":"2024-06-30T11:20:28.918627Z","iopub.status.idle":"2024-06-30T11:20:28.925236Z","shell.execute_reply.started":"2024-06-30T11:20:28.918591Z","shell.execute_reply":"2024-06-30T11:20:28.924339Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Define Accuracy Metric","metadata":{}},{"cell_type":"code","source":"accuracy_metric = load_metric(\"accuracy\")\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy_metric.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:20:28.926600Z","iopub.execute_input":"2024-06-30T11:20:28.926927Z","iopub.status.idle":"2024-06-30T11:20:30.115632Z","shell.execute_reply.started":"2024-06-30T11:20:28.926898Z","shell.execute_reply":"2024-06-30T11:20:30.114853Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_35/3547444929.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  accuracy_metric = load_metric(\"accuracy\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/accuracy/accuracy.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a76aea8718854ccf95cc71eb33ca8643"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Saving Checkpoint","metadata":{}},{"cell_type":"code","source":"class CustomSaveCallback(TrainerCallback):\n    def on_save(self, args, state, control, **kwargs):\n        try:\n            # Define the output directory in Google Drive\n            output_dir = '/content/drive/MyDrive/Wav2Vec2_Checkpoints'\n            model = kwargs['model']\n            optimizer = kwargs['optimizer']\n            \n            # Save model checkpoint\n            model_path = os.path.join(output_dir, \"pytorch_model.bin\")\n            optimizer_path = os.path.join(output_dir, \"optimizer.pt\")\n\n            if not os.path.exists(output_dir):\n                os.makedirs(output_dir)\n                \n            torch.save(model.state_dict(), model_path)\n            torch.save(optimizer.state_dict(), optimizer_path)\n            \n            print(f\"Checkpoint saved successfully at {output_dir}\")\n        except Exception as e:\n            print(f\"Error saving checkpoint: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:22:39.369203Z","iopub.execute_input":"2024-06-30T11:22:39.370159Z","iopub.status.idle":"2024-06-30T11:22:39.377429Z","shell.execute_reply.started":"2024-06-30T11:22:39.370116Z","shell.execute_reply":"2024-06-30T11:22:39.376357Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Custom Class to visualize learning curve","metadata":{}},{"cell_type":"code","source":"# Logging callback to record metrics\nclass LoggingCallback(TrainerCallback):\n    def __init__(self):\n        self.train_loss = []\n        self.eval_loss = []\n        self.train_accuracy = []\n        self.eval_accuracy = []\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs is not None:\n            self.train_loss.append(logs.get(\"loss\"))\n            self.eval_loss.append(logs.get(\"eval_loss\"))\n            self.train_accuracy.append(logs.get(\"train_accuracy\"))\n            self.eval_accuracy.append(logs.get(\"eval_accuracy\"))\n\n    def plot_metrics(self):\n        epochs = range(1, len(self.train_loss) + 1)\n        \n        plt.figure(figsize=(14, 7))\n        plt.subplot(1, 2, 1)\n        plt.plot(epochs, self.train_loss, label=\"Training Loss\")\n        plt.plot(epochs, self.eval_loss, label=\"Validation Loss\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Training and Validation Loss\")\n        plt.legend()\n        \n        plt.subplot(1, 2, 2)\n        plt.plot(epochs, self.train_accuracy, label=\"Training Accuracy\")\n        plt.plot(epochs, self.eval_accuracy, label=\"Validation Accuracy\")\n        plt.xlabel(\"Epochs\")\n        plt.ylabel(\"Accuracy\")\n        plt.title(\"Training and Validation Accuracy\")\n        plt.legend()\n        \n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:23:02.838510Z","iopub.execute_input":"2024-06-30T11:23:02.838901Z","iopub.status.idle":"2024-06-30T11:23:02.849627Z","shell.execute_reply.started":"2024-06-30T11:23:02.838871Z","shell.execute_reply":"2024-06-30T11:23:02.848649Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Load the wavtovec pretrained model","metadata":{}},{"cell_type":"code","source":"model = Wav2Vec2ForSequenceClassification.from_pretrained(\n    \"elgeish/wav2vec2-large-xlsr-53-arabic\",\n    num_labels=len(label2id),\n    ignore_mismatched_sizes=True,\n    problem_type=\"single_label_classification\",\n)\n\n# Initialize the logging callback\nlogging_callback = LoggingCallback()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:23:19.368903Z","iopub.execute_input":"2024-06-30T11:23:19.369286Z","iopub.status.idle":"2024-06-30T11:24:33.859722Z","shell.execute_reply.started":"2024-06-30T11:23:19.369254Z","shell.execute_reply":"2024-06-30T11:24:33.858956Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f44c1dcab504f299c9902ff644f7eb4"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at elgeish/wav2vec2-large-xlsr-53-arabic and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Define training arguments and trainer","metadata":{}},{"cell_type":"code","source":"# Define your training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/content/drive/MyDrive/Wav2Vec2_Checkpoints\",               # Directory to save checkpoints\n    weight_decay=0.01,\n    learning_rate = 2e-5,\n    num_train_epochs= 15,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    logging_dir=\"/content/drive/MyDrive/Wav2Vec2_Logs\",                 # Directory for logging\n    logging_steps=10,\n    eval_strategy=\"epoch\",          # Evaluate at the end of each epoch\n    save_strategy=\"epoch\",                # Save checkpoint at the end of each epoch\n    save_total_limit=2,                   # Limit the total number of checkpoints\n    load_best_model_at_end=True,          # Load the best model at the end\n    metric_for_best_model=\"accuracy\",     # Metric to use to select the best model\n    greater_is_better=True,               # Whether a greater metric value is better\n    fp16=True,\n    run_name = \"Egyptain Arabic SER\"\n)\n\n# Define the data collator\ndef custom_data_collator(features):\n    batch = {k: [f[k] for f in features] for k in features[0].keys()}\n    labels = torch.tensor(batch.pop('labels'), dtype=torch.long)\n    inputs = processor(batch['input_values'], sampling_rate=16000, return_tensors=\"pt\", padding=True)\n    inputs['labels'] = labels\n    return inputs\n\n# Initialize the Trainer\ntrainer =CustomTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=custom_data_collator,\n    tokenizer=processor.feature_extractor,  # or processor if feature_extractor is not available\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3), logging_callback],\n    class_weights=class_weights_tensor.tolist(),  # Add class weights to training arguments\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:24:33.861259Z","iopub.execute_input":"2024-06-30T11:24:33.861557Z","iopub.status.idle":"2024-06-30T11:24:35.524159Z","shell.execute_reply.started":"2024-06-30T11:24:33.861532Z","shell.execute_reply":"2024-06-30T11:24:35.523433Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T11:25:21.959520Z","iopub.execute_input":"2024-06-30T11:25:21.959898Z","iopub.status.idle":"2024-06-30T15:31:49.942787Z","shell.execute_reply.started":"2024-06-30T11:25:21.959867Z","shell.execute_reply":"2024-06-30T15:31:49.941511Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240630_112648-ut764t05</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/janag-attia-Ain%20Shams%20University/huggingface/runs/ut764t05' target=\"_blank\">Egyptain Arabic SER</a></strong> to <a href='https://wandb.ai/janag-attia-Ain%20Shams%20University/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/janag-attia-Ain%20Shams%20University/huggingface' target=\"_blank\">https://wandb.ai/janag-attia-Ain%20Shams%20University/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/janag-attia-Ain%20Shams%20University/huggingface/runs/ut764t05' target=\"_blank\">https://wandb.ai/janag-attia-Ain%20Shams%20University/huggingface/runs/ut764t05</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4340' max='4650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4340/4650 4:04:37 < 17:28, 0.30 it/s, Epoch 14/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>1.247500</td>\n      <td>0.940551</td>\n      <td>0.621469</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.662700</td>\n      <td>0.831629</td>\n      <td>0.640301</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.760000</td>\n      <td>0.734657</td>\n      <td>0.702448</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.734800</td>\n      <td>0.768250</td>\n      <td>0.694915</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.652800</td>\n      <td>0.832416</td>\n      <td>0.696798</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.279600</td>\n      <td>1.083706</td>\n      <td>0.715631</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.162200</td>\n      <td>1.580787</td>\n      <td>0.708098</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.893600</td>\n      <td>2.878526</td>\n      <td>0.719397</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.289500</td>\n      <td>4.076486</td>\n      <td>0.728814</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.748900</td>\n      <td>4.408115</td>\n      <td>0.721281</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.541200</td>\n      <td>4.111590</td>\n      <td>0.734463</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.302600</td>\n      <td>4.433111</td>\n      <td>0.725047</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.396400</td>\n      <td>4.411978</td>\n      <td>0.725047</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.206800</td>\n      <td>4.436864</td>\n      <td>0.725047</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4340, training_loss=0.4688099199411175, metrics={'train_runtime': 14787.5746, 'train_samples_per_second': 2.514, 'train_steps_per_second': 0.314, 'total_flos': 1.2168970815461659e+19, 'train_loss': 0.4688099199411175, 'epoch': 14.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"# Save best Model","metadata":{}},{"cell_type":"code","source":"# Save the best model\ntorch.save(model.state_dict(), '/kaggle/working/best_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-30T21:17:51.548664Z","iopub.execute_input":"2024-06-30T21:17:51.549014Z","iopub.status.idle":"2024-06-30T21:17:51.883446Z","shell.execute_reply.started":"2024-06-30T21:17:51.548984Z","shell.execute_reply":"2024-06-30T21:17:51.882256Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the best model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/best_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"],"ename":"NameError","evalue":"name 'torch' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Plot Learning Curve","metadata":{}},{"cell_type":"code","source":"# Plot the metrics after training\nlogging_callback.plot_metrics()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Model","metadata":{}},{"cell_type":"code","source":"# Evaluate on the validation dataset\nval_results = trainer.evaluate(eval_dataset=val_dataset)\nval_accuracy = val_results['eval_accuracy']\nprint(f\"Validation Accuracy: {val_accuracy:.4f}\")\n\n# Evaluate on the test dataset\ntest_results = trainer.predict(test_dataset)\ntest_accuracy = test_results.metrics['test_accuracy']\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\n# Predict on the test dataset\npredictions = trainer.predict(test_dataset).predictions\npredicted_labels = predictions.argmax(axis=1)\ntrue_labels = test_dataset[\"labels\"]\n\n# Compute the classification report\nreport = classification_report(true_labels, predicted_labels, target_names=[\"neu\", \"ang\", \"sad\", \"hap\"])\n\n# Display the classification report\nprint(\"Classification Report:\")\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:36:28.479665Z","iopub.status.idle":"2024-06-30T19:36:28.479992Z","shell.execute_reply.started":"2024-06-30T19:36:28.479830Z","shell.execute_reply":"2024-06-30T19:36:28.479844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Display Confusion matrix","metadata":{}},{"cell_type":"code","source":"# Define your emotions based on your label mapping\nemotions = ['happy', 'sad', 'angry', 'neutral']\n\n# Function to plot confusion matrix\ndef plot_confusion_matrix(conf_matrix, title):\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=emotions, yticklabels=emotions)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.title(title)\n    plt.show()\n\n\n# Assuming your predictions are in `test_results.predictions`\ntest_predictions = np.argmax(test_results.predictions, axis=1)\ntest_labels = test_dataset[\"labels\"]\n\n# Calculate confusion matrix\ntest_conf_matrix = confusion_matrix(test_labels, test_predictions)\n\n# Plot Test Confusion Matrix\nprint(\"Test Confusion Matrix:\")\nplot_confusion_matrix(test_conf_matrix, title='Test Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:36:28.494366Z","iopub.execute_input":"2024-06-30T19:36:28.494637Z","iopub.status.idle":"2024-06-30T19:36:28.548910Z","shell.execute_reply.started":"2024-06-30T19:36:28.494613Z","shell.execute_reply":"2024-06-30T19:36:28.547460Z"},"trusted":true},"execution_count":32,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Assuming your predictions are in `test_results.predictions`\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mtest_results\u001b[49m\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m test_dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate confusion matrix\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'test_results' is not defined"],"ename":"NameError","evalue":"name 'test_results' is not defined","output_type":"error"}]},{"cell_type":"code","source":"\"\"\"# Function to compute emotion percentages\ndef compute_emotion_percentages(model, tokenizer, audio_file_path):\n    inputs = tokenizer(audio_file_path, return_tensors=\"pt\", sampling_rate=16000, padding=True)\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        probabilities = torch.nn.functional.softmax(logits, dim=-1).cpu().numpy().flatten()\n        percentages = {emotion: round(prob * 100, 2) for emotion, prob in zip([\"neutral\", \"angry\", \"sad\", \"happy\"], probabilities)}\n    return percentages\n\n# Example usage\naudio_file_path = \"path_to_audio_file.wav\"  # Replace with your audio file path\nemotion_percentages = compute_emotion_percentages(model, tokenizer, audio_file_path)\nprint(emotion_percentages)\"\"\"","metadata":{},"execution_count":null,"outputs":[]}]}